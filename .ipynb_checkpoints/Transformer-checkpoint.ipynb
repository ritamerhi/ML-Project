{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02189a53",
   "metadata": {},
   "source": [
    "## plan:\n",
    "- learn the basics of gold, silver and CAD prices\n",
    "- try a simple linear regression just to say that we tried it\n",
    "- experiment with LSTMs\n",
    "- account for inflation and other economic factors that may be relevant\n",
    "- scrape news headlines and use them for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa4c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbf109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date: 2025-10-01 00:52:17.334073\n",
      "End date: 2025-11-30 00:52:17.334073\n"
     ]
    }
   ],
   "source": [
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=60)\n",
    "\n",
    "print(\"Start date:\", start_date)\n",
    "print(\"End date:\", end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da02cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# my_end = \"2025-11-20\" # so I can use the days after that for testing\n",
    "# end_date = datetime.strptime(my_end, \"%Y-%m-%d\")\n",
    "# start_date = \"2025-10-01\"\n",
    "\n",
    "gold = yf.download(\"GC=F\", start = start_date, end = end_date, interval = \"30m\", auto_adjust = False)\n",
    "silver = yf.download(\"SI=F\", start = start_date, end = end_date, interval = \"30m\",auto_adjust = False)\n",
    "cad = yf.download(\"CADUSD=X\", start = start_date, end = end_date, interval = \"30m\", auto_adjust = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2c49c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = gold[[\"Close\"]].rename(columns={\"Close\": \"Gold\"})\n",
    "silver = silver[[\"Close\"]].rename(columns={\"Close\": \"Silver\"})\n",
    "cad = cad[[\"Close\"]].rename(columns={\"Close\": \"CAD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d12cc98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prices = gold.join([silver, cad], how=\"outer\")\n",
    "all_prices.to_csv(\"prices_with_null.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dad80a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prices = all_prices.ffill().bfill() # forward-fill + backward-fill to deal with missing values\n",
    "all_prices.to_csv(\"prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b62d35c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "      <th>CAD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>GC=F</th>\n",
       "      <th>SI=F</th>\n",
       "      <th>CADUSD=X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-09-30 23:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 00:00:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 00:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 01:00:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 01:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                             Gold     Silver       CAD\n",
       "Ticker                            GC=F       SI=F  CADUSD=X\n",
       "Datetime                                                   \n",
       "2025-09-30 23:30:00+00:00  3892.600098  47.325001  0.718200\n",
       "2025-10-01 00:00:00+00:00  3892.600098  47.325001  0.718066\n",
       "2025-10-01 00:30:00+00:00  3892.600098  47.325001  0.718045\n",
       "2025-10-01 01:00:00+00:00  3892.600098  47.325001  0.718174\n",
       "2025-10-01 01:30:00+00:00  3892.600098  47.325001  0.718231"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "440df3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2053 entries, 2025-09-30 23:30:00+00:00 to 2025-11-28 23:00:00+00:00\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   (Gold, GC=F)     2053 non-null   float64\n",
      " 1   (Silver, SI=F)   2053 non-null   float64\n",
      " 2   (CAD, CADUSD=X)  2053 non-null   float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 64.2 KB\n"
     ]
    }
   ],
   "source": [
    "all_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cad9296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "      <th>CAD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>GC=F</th>\n",
       "      <th>SI=F</th>\n",
       "      <th>CADUSD=X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2053.000000</td>\n",
       "      <td>2053.000000</td>\n",
       "      <td>2053.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4083.217005</td>\n",
       "      <td>49.639613</td>\n",
       "      <td>0.713281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>109.418956</td>\n",
       "      <td>2.088170</td>\n",
       "      <td>0.002607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3851.800049</td>\n",
       "      <td>45.665001</td>\n",
       "      <td>0.707269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4004.000000</td>\n",
       "      <td>47.970001</td>\n",
       "      <td>0.711805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4072.100098</td>\n",
       "      <td>48.884998</td>\n",
       "      <td>0.713267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4153.899902</td>\n",
       "      <td>51.029999</td>\n",
       "      <td>0.714944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4394.299805</td>\n",
       "      <td>57.080002</td>\n",
       "      <td>0.719746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price          Gold       Silver          CAD\n",
       "Ticker         GC=F         SI=F     CADUSD=X\n",
       "count   2053.000000  2053.000000  2053.000000\n",
       "mean    4083.217005    49.639613     0.713281\n",
       "std      109.418956     2.088170     0.002607\n",
       "min     3851.800049    45.665001     0.707269\n",
       "25%     4004.000000    47.970001     0.711805\n",
       "50%     4072.100098    48.884998     0.713267\n",
       "75%     4153.899902    51.029999     0.714944\n",
       "max     4394.299805    57.080002     0.719746"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prices.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8689950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold GC=F</th>\n",
       "      <th>Silver SI=F</th>\n",
       "      <th>CAD CADUSD=X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-09-30 23:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 00:00:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 00:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 01:00:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 01:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Gold GC=F  Silver SI=F  CAD CADUSD=X\n",
       "Datetime                                                         \n",
       "2025-09-30 23:30:00+00:00  3892.600098    47.325001      0.718200\n",
       "2025-10-01 00:00:00+00:00  3892.600098    47.325001      0.718066\n",
       "2025-10-01 00:30:00+00:00  3892.600098    47.325001      0.718045\n",
       "2025-10-01 01:00:00+00:00  3892.600098    47.325001      0.718174\n",
       "2025-10-01 01:30:00+00:00  3892.600098    47.325001      0.718231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the multi-index columns\n",
    "all_prices.columns = [' '.join(col).strip() for col in all_prices.columns.values]\n",
    "\n",
    "all_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c09a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prices = all_prices.rename(columns={\n",
    "    'Gold GC=F': 'Gold',\n",
    "    'Silver SI=F': 'Silver',\n",
    "    'CAD CADUSD=X': 'CAD'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78e5e03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "      <th>CAD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-09-30 23:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 00:00:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 00:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 01:00:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 01:30:00+00:00</th>\n",
       "      <td>3892.600098</td>\n",
       "      <td>47.325001</td>\n",
       "      <td>0.718231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Gold     Silver       CAD\n",
       "Datetime                                                   \n",
       "2025-09-30 23:30:00+00:00  3892.600098  47.325001  0.718200\n",
       "2025-10-01 00:00:00+00:00  3892.600098  47.325001  0.718066\n",
       "2025-10-01 00:30:00+00:00  3892.600098  47.325001  0.718045\n",
       "2025-10-01 01:00:00+00:00  3892.600098  47.325001  0.718174\n",
       "2025-10-01 01:30:00+00:00  3892.600098  47.325001  0.718231"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c926e1",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec893e9",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e15fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cbe724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window):\n",
    "        X.append(data[i : i + window])\n",
    "        y.append(data[i + window])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b258898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_predict(X):\n",
    "    return X[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d4e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_ratio=0.2):\n",
    "    split = int(len(X) * (1 - test_ratio))\n",
    "    return X[:split], X[split:], y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63342538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(window, features=3):\n",
    "    model = models.Sequential([\n",
    "        layers.Input((window, features)),\n",
    "        layers.LSTM(64, return_sequences=False),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(features) \n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse'\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "947d443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Window = 10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764456049.347698  113632 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10065 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2025-11-30 00:40:50.482425: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Gold:\n",
      "  Model      → RMSE: 11.1630 | MAE: 7.9792 | MAPE: 0.19%\n",
      "  Baseline   → RMSE: 10.0473 | MAE: 5.7122 | MAPE: 0.14%\n",
      "Silver:\n",
      "  Model      → RMSE: 0.3952 | MAE: 0.2930 | MAPE: 0.57%\n",
      "  Baseline   → RMSE: 0.2145 | MAE: 0.1182 | MAPE: 0.23%\n",
      "CAD:\n",
      "  Model      → RMSE: 0.0004 | MAE: 0.0003 | MAPE: 0.04%\n",
      "  Baseline   → RMSE: 0.0003 | MAE: 0.0002 | MAPE: 0.03%\n",
      "\n",
      "\n",
      "=== Window = 20 ===\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Gold:\n",
      "  Model      → RMSE: 11.7884 | MAE: 8.4966 | MAPE: 0.21%\n",
      "  Baseline   → RMSE: 10.0676 | MAE: 5.7219 | MAPE: 0.14%\n",
      "Silver:\n",
      "  Model      → RMSE: 0.4293 | MAE: 0.3004 | MAPE: 0.57%\n",
      "  Baseline   → RMSE: 0.2150 | MAE: 0.1185 | MAPE: 0.23%\n",
      "CAD:\n",
      "  Model      → RMSE: 0.0005 | MAE: 0.0003 | MAPE: 0.04%\n",
      "  Baseline   → RMSE: 0.0003 | MAE: 0.0002 | MAPE: 0.03%\n",
      "\n",
      "\n",
      "=== Window = 30 ===\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Gold:\n",
      "  Model      → RMSE: 11.7402 | MAE: 8.1706 | MAPE: 0.20%\n",
      "  Baseline   → RMSE: 10.0804 | MAE: 5.7175 | MAPE: 0.14%\n",
      "Silver:\n",
      "  Model      → RMSE: 0.3313 | MAE: 0.2308 | MAPE: 0.45%\n",
      "  Baseline   → RMSE: 0.2153 | MAE: 0.1186 | MAPE: 0.23%\n",
      "CAD:\n",
      "  Model      → RMSE: 0.0004 | MAE: 0.0003 | MAPE: 0.04%\n",
      "  Baseline   → RMSE: 0.0003 | MAE: 0.0002 | MAPE: 0.03%\n",
      "\n",
      "\n",
      "=== Window = 40 ===\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Gold:\n",
      "  Model      → RMSE: 11.6967 | MAE: 8.7527 | MAPE: 0.21%\n",
      "  Baseline   → RMSE: 10.1000 | MAE: 5.7278 | MAPE: 0.14%\n",
      "Silver:\n",
      "  Model      → RMSE: 0.3392 | MAE: 0.2254 | MAPE: 0.44%\n",
      "  Baseline   → RMSE: 0.2153 | MAE: 0.1182 | MAPE: 0.23%\n",
      "CAD:\n",
      "  Model      → RMSE: 0.0004 | MAE: 0.0003 | MAPE: 0.04%\n",
      "  Baseline   → RMSE: 0.0003 | MAE: 0.0002 | MAPE: 0.03%\n",
      "\n",
      "\n",
      "=== Window = 60 ===\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Gold:\n",
      "  Model      → RMSE: 12.6698 | MAE: 9.6748 | MAPE: 0.23%\n",
      "  Baseline   → RMSE: 10.0569 | MAE: 5.6521 | MAPE: 0.14%\n",
      "Silver:\n",
      "  Model      → RMSE: 0.3140 | MAE: 0.1960 | MAPE: 0.38%\n",
      "  Baseline   → RMSE: 0.2153 | MAE: 0.1175 | MAPE: 0.23%\n",
      "CAD:\n",
      "  Model      → RMSE: 0.0005 | MAE: 0.0003 | MAPE: 0.04%\n",
      "  Baseline   → RMSE: 0.0003 | MAE: 0.0002 | MAPE: 0.03%\n",
      "\n",
      "\n",
      "=== Window = 90 ===\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Gold:\n",
      "  Model      → RMSE: 12.0767 | MAE: 9.2422 | MAPE: 0.22%\n",
      "  Baseline   → RMSE: 10.0797 | MAE: 5.6313 | MAPE: 0.14%\n",
      "Silver:\n",
      "  Model      → RMSE: 0.4794 | MAE: 0.3639 | MAPE: 0.70%\n",
      "  Baseline   → RMSE: 0.2157 | MAE: 0.1170 | MAPE: 0.23%\n",
      "CAD:\n",
      "  Model      → RMSE: 0.0005 | MAE: 0.0004 | MAPE: 0.05%\n",
      "  Baseline   → RMSE: 0.0003 | MAE: 0.0002 | MAPE: 0.03%\n",
      "\n",
      "\n",
      "=== Window = 120 ===\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Gold:\n",
      "  Model      → RMSE: 12.3446 | MAE: 8.5637 | MAPE: 0.21%\n",
      "  Baseline   → RMSE: 10.1290 | MAE: 5.6295 | MAPE: 0.14%\n",
      "Silver:\n",
      "  Model      → RMSE: 0.3338 | MAE: 0.2297 | MAPE: 0.44%\n",
      "  Baseline   → RMSE: 0.2163 | MAE: 0.1164 | MAPE: 0.23%\n",
      "CAD:\n",
      "  Model      → RMSE: 0.0005 | MAE: 0.0004 | MAPE: 0.05%\n",
      "  Baseline   → RMSE: 0.0003 | MAE: 0.0002 | MAPE: 0.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values = all_prices.values \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "window_sizes = [10, 20, 30, 40, 60, 90, 120]\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for window in window_sizes:\n",
    "    print(f\"\\n Window = {window}\")\n",
    "\n",
    "    X, y = create_sequences(scaled, window)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = build_model(window)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20, batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_base = baseline_predict(X_test)\n",
    "\n",
    "    y_test_inv = scaler.inverse_transform(y_test)\n",
    "    y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "    y_base_inv = scaler.inverse_transform(y_base)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for i, asset in enumerate([\"Gold\", \"Silver\", \"CAD\"]):\n",
    "        rmse_m = rmse(y_test_inv[:, i], y_pred_inv[:, i])\n",
    "        mae_m  = mae (y_test_inv[:, i], y_pred_inv[:, i])\n",
    "        mape_m = mape(y_test_inv[:, i], y_pred_inv[:, i])\n",
    "\n",
    "        rmse_b = rmse(y_test_inv[:, i], y_base_inv[:, i])\n",
    "        mae_b  = mae (y_test_inv[:, i], y_base_inv[:, i])\n",
    "        mape_b = mape(y_test_inv[:, i], y_base_inv[:, i])\n",
    "\n",
    "        metrics[asset] = {\n",
    "            \"RMSE\": rmse_m, \"MAE\": mae_m, \"MAPE\": mape_m,\n",
    "            \"Base_RMSE\": rmse_b, \"Base_MAE\": mae_b, \"Base_MAPE\": mape_b,\n",
    "        }\n",
    "\n",
    "    results.append((window, metrics))\n",
    "\n",
    "    for asset in metrics:\n",
    "        print(f\"{asset}:\")\n",
    "        print(\"  Model      → RMSE: {:.4f} | MAE: {:.4f} | MAPE: {:.2f}%\".format(\n",
    "            metrics[asset][\"RMSE\"], metrics[asset][\"MAE\"], metrics[asset][\"MAPE\"]))\n",
    "        print(\"  Baseline   → RMSE: {:.4f} | MAE: {:.4f} | MAPE: {:.2f}%\".format(\n",
    "            metrics[asset][\"Base_RMSE\"], metrics[asset][\"Base_MAE\"], metrics[asset][\"Base_MAPE\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68006508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_features(index: pd.DatetimeIndex) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build cyclical time features from a DatetimeIndex.\n",
    "    Returns array of shape (len(index), 8)\n",
    "    \"\"\"\n",
    "    # basic components\n",
    "    hour = index.hour.values\n",
    "    minute = index.minute.values\n",
    "    dayofweek = index.dayofweek.values      # 0–6\n",
    "    dayofyear = index.dayofyear.values      # 1–366\n",
    "    \n",
    "    # cyclical encodings\n",
    "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "    minute_sin = np.sin(2 * np.pi * minute / 60)\n",
    "    minute_cos = np.cos(2 * np.pi * minute / 60)\n",
    "\n",
    "    dow_sin = np.sin(2 * np.pi * dayofweek / 7)\n",
    "    dow_cos = np.cos(2 * np.pi * dayofweek / 7)\n",
    "\n",
    "    doy_sin = np.sin(2 * np.pi * dayofyear / 365)\n",
    "    doy_cos = np.cos(2 * np.pi * dayofyear / 365)\n",
    "    \n",
    "    time_feats = np.stack(\n",
    "        [hour_sin, hour_cos,\n",
    "         minute_sin, minute_cos,\n",
    "         dow_sin, dow_cos,\n",
    "         doy_sin, doy_cos],\n",
    "        axis=1\n",
    "    )\n",
    "    return time_feats  # (N, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04cc1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def make_seq2seq_data(all_prices, enc_len, dec_len):\n",
    "    \"\"\"\n",
    "    all_prices: DataFrame with columns [Gold, Silver, CAD] and DateTimeIndex\n",
    "    enc_len: encoder history length (in steps)\n",
    "    dec_len: decoder forecast length (in steps)\n",
    "    \n",
    "    Returns:\n",
    "        enc_inputs: (num_samples, enc_len, num_features)\n",
    "        dec_inputs: (num_samples, dec_len, num_features)\n",
    "        targets:    (num_samples, dec_len, 3)   # only prices\n",
    "        scaler:     fitted MinMaxScaler for prices\n",
    "    \"\"\"\n",
    "    values = all_prices[[\"Gold\", \"Silver\", \"CAD\"]].values.astype(\"float32\")\n",
    "    time_feats = build_time_features(all_prices.index)  # (N, 8)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_prices = scaler.fit_transform(values)        # (N, 3)\n",
    "\n",
    "    N = len(all_prices)\n",
    "    num_samples = N - enc_len - dec_len\n",
    "    if num_samples <= 0:\n",
    "        raise ValueError(\"Not enough data for given enc_len and dec_len.\")\n",
    "\n",
    "    enc_list, dec_list, tgt_list = [], [], []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # indices\n",
    "        enc_start = i\n",
    "        enc_end   = i + enc_len\n",
    "        dec_end   = enc_end + dec_len\n",
    "\n",
    "        # slices\n",
    "        past_prices = scaled_prices[enc_start:enc_end]          # (enc_len, 3)\n",
    "        past_time   = time_feats[enc_start:enc_end]             # (enc_len, 8)\n",
    "\n",
    "        future_prices = scaled_prices[enc_end:dec_end]          # (dec_len, 3)\n",
    "        future_time   = time_feats[enc_end:dec_end]             # (dec_len, 8)\n",
    "\n",
    "        # encoder input: past prices + time feats\n",
    "        enc_in = np.concatenate([past_prices, past_time], axis=1)   # (enc_len, 11)\n",
    "\n",
    "        # decoder input:\n",
    "        # teacher forcing: input_t at step k = true price at k-1 (first one uses last encoder price)\n",
    "        first_price = past_prices[-1:]          # shape (1, 3)\n",
    "        shifted_prices = np.concatenate([first_price, future_prices[:-1]], axis=0)  # (dec_len, 3)\n",
    "        dec_in = np.concatenate([shifted_prices, future_time], axis=1)              # (dec_len, 11)\n",
    "\n",
    "        # target: future prices (not including time feats)\n",
    "        tgt = future_prices   # (dec_len, 3)\n",
    "\n",
    "        enc_list.append(enc_in)\n",
    "        dec_list.append(dec_in)\n",
    "        tgt_list.append(tgt)\n",
    "\n",
    "    enc_inputs = np.stack(enc_list)  # (samples, enc_len, 11)\n",
    "    dec_inputs = np.stack(dec_list)  # (samples, dec_len, 11)\n",
    "    targets    = np.stack(tgt_list)  # (samples, dec_len, 3)\n",
    "\n",
    "    return enc_inputs, dec_inputs, targets, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67457578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def transformer_encoder(x, num_layers, d_model, num_heads, dff, dropout):\n",
    "    for _ in range(num_layers):\n",
    "        # Self-attention\n",
    "        attn_out = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model\n",
    "        )(x, x)\n",
    "        x = layers.Add()([x, attn_out])\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn = layers.Dense(dff, activation=\"relu\")(x)\n",
    "        ffn = layers.Dense(d_model)(ffn)\n",
    "        x = layers.Add()([x, ffn])\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def transformer_decoder(y, enc_output, num_layers, d_model, num_heads, dff, dropout):\n",
    "    for _ in range(num_layers):\n",
    "        # 1) masked self-attention on decoder\n",
    "        self_attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model\n",
    "        )(y, y, use_causal_mask=True)\n",
    "        y = layers.Add()([y, self_attn])\n",
    "        y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "\n",
    "        # 2) cross-attention over encoder outputs\n",
    "        cross_attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model\n",
    "        )(y, enc_output)\n",
    "        y = layers.Add()([y, cross_attn])\n",
    "        y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "\n",
    "        # 3) feed-forward\n",
    "        ffn = layers.Dense(dff, activation=\"relu\")(y)\n",
    "        ffn = layers.Dense(d_model)(ffn)\n",
    "        y = layers.Add()([y, ffn])\n",
    "        y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "\n",
    "        y = layers.Dropout(dropout)(y)\n",
    "    return y\n",
    "\n",
    "def build_transformer_seq2seq(\n",
    "    enc_len,\n",
    "    dec_len,\n",
    "    num_features,  # prices + time feats = 11\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    dff=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "):\n",
    "    # encoder input\n",
    "    enc_inputs = layers.Input(shape=(enc_len, num_features), name=\"encoder_inputs\")\n",
    "    # decoder input\n",
    "    dec_inputs = layers.Input(shape=(dec_len, num_features), name=\"decoder_inputs\")\n",
    "\n",
    "    # project input features to d_model\n",
    "    enc_proj = layers.Dense(d_model)(enc_inputs)\n",
    "    dec_proj = layers.Dense(d_model)(dec_inputs)\n",
    "\n",
    "    # encoder\n",
    "    enc_output = transformer_encoder(enc_proj, num_layers, d_model, num_heads, dff, dropout)\n",
    "\n",
    "    # decoder\n",
    "    dec_output = transformer_decoder(dec_proj, enc_output, num_layers, d_model, num_heads, dff, dropout)\n",
    "\n",
    "    # final prediction: 3 outputs (Gold, Silver, CAD) per decoder timestep\n",
    "    outputs = layers.Dense(3, name=\"prices\")(dec_output)\n",
    "\n",
    "    model = Model([enc_inputs, dec_inputs], outputs, name=\"transformer_seq2seq\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "462c066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764457069.523672  134221 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10065 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 00:57:54.033205: I external/local_xla/xla/service/service.cc:163] XLA service 0x73d77800fc20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-30 00:57:54.033222: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 Laptop GPU, Compute Capability 8.9\n",
      "2025-11-30 00:57:54.175797: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-30 00:57:55.023266: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91400\n",
      "2025-11-30 00:57:55.463003: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463109: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463193: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463209: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463251: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463267: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463279: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463498: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463764: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463826: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:55.463849: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:57:56.386507: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:56.392795: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:56.667738: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.044177: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.258139: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_84', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.274075: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8169', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.445245: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8169', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.475875: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 244 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.645352: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_48', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.725232: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8028', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.791209: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8968', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.887270: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_107', 100 bytes spill stores, 100 bytes spill loads\n",
      "\n",
      "2025-11-30 00:57:57.904033: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/49\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0837 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764457084.744403  134517 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8398"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 00:58:06.129986: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130069: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130083: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130138: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130152: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130191: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130205: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130217: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130381: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130395: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130406: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130530: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130575: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130623: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.130641: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:06.836934: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:06.872310: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 360 bytes spill stores, 364 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.003849: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.079643: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8', 360 bytes spill stores, 360 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.084652: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.114356: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.153138: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_48', 580 bytes spill stores, 580 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.254284: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.306879: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 360 bytes spill stores, 360 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.415013: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_84', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.541981: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 592 bytes spill stores, 596 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.948588: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:07.976615: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8169', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:08.068307: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_107', 100 bytes spill stores, 100 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:08.484751: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8968', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:08.620261: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8169', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:08.623373: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8980', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 00:58:08.699896: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_86', 592 bytes spill stores, 596 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:08.872679: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 244 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:08.963252: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8028', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:09.007453: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8980', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:09.013948: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_84', 580 bytes spill stores, 580 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - loss: 0.8105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 00:58:18.508894: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.508943: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.508965: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.509008: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.509017: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.509030: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.509042: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.509064: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.509072: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.509083: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-30 00:58:18.916944: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.264103: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 360 bytes spill stores, 364 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.334166: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 320 bytes spill stores, 320 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.415417: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8', 360 bytes spill stores, 360 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.419771: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.506761: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.524364: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.532017: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.637794: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.831279: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.867855: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.870805: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.927062: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_48', 352 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:19.963765: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 320 bytes spill stores, 320 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:20.013175: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:20.150562: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:20.150590: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:20.177307: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 360 bytes spill stores, 360 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:20.222237: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:20.249821: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 352 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:20.284419: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-11-30 00:58:20.303368: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_48', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 360ms/step - loss: 0.3582 - val_loss: 0.0204\n",
      "Epoch 2/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0952 - val_loss: 0.0095\n",
      "Epoch 3/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0472 - val_loss: 0.0109\n",
      "Epoch 4/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0239 - val_loss: 0.0079\n",
      "Epoch 5/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0146 - val_loss: 0.0071\n",
      "Epoch 6/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0105 - val_loss: 0.0057\n",
      "Epoch 7/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0080 - val_loss: 0.0050\n",
      "Epoch 8/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0067 - val_loss: 0.0045\n",
      "Epoch 9/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0056 - val_loss: 0.0040\n",
      "Epoch 10/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0050 - val_loss: 0.0041\n",
      "Epoch 11/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - val_loss: 0.0034\n",
      "Epoch 12/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0035\n",
      "Epoch 13/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 14/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 15/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0032 - val_loss: 0.0026\n",
      "Epoch 16/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0030 - val_loss: 0.0022\n",
      "Epoch 17/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0027 - val_loss: 0.0021\n",
      "Epoch 18/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 19/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 20/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 21/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 22/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 23/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 24/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 25/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 26/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 27/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 28/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 29/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 30/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 31/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 32/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 33/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 34/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 35/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 36/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 37/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 38/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 39/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 40/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 41/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 42/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 43/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 44/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 45/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 46/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 47/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 48/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 49/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 50/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - val_loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "# choose lengths\n",
    "ENC_LEN = 60   # past 60 steps = 30 hours\n",
    "DEC_LEN = 1   # forecast next 48 steps = 24 hours (for example)\n",
    "# If you really want 30min horizon only, set DEC_LEN = 1\n",
    "\n",
    "enc_in, dec_in, targets, scaler = make_seq2seq_data(all_prices, ENC_LEN, DEC_LEN)\n",
    "\n",
    "# train/test split (time-based)\n",
    "split = int(len(enc_in) * 0.8)\n",
    "X_enc_train, X_enc_test = enc_in[:split], enc_in[split:]\n",
    "X_dec_train, X_dec_test = dec_in[:split], dec_in[split:]\n",
    "y_train, y_test         = targets[:split], targets[split:]\n",
    "\n",
    "num_features = enc_in.shape[-1]  # should be 11\n",
    "\n",
    "model = build_transformer_seq2seq(\n",
    "    enc_len=ENC_LEN,\n",
    "    dec_len=DEC_LEN,\n",
    "    num_features=num_features\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_enc_train, X_dec_train], y_train,\n",
    "    validation_data=([X_enc_test, X_dec_test], y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15c39ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step\n",
      "Gold → RMSE: 14.2299 | MAE: 10.7272 | MAPE: 0.26%\n",
      "Silver → RMSE: 0.5359 | MAE: 0.4050 | MAPE: 0.79%\n",
      "CAD → RMSE: 0.0005 | MAE: 0.0004 | MAPE: 0.05%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([X_enc_test, X_dec_test])\n",
    "\n",
    "# reshape to 2D for inverse scaling: (samples*DEC_LEN, 3)\n",
    "y_test_2d = y_test.reshape(-1, 3)\n",
    "y_pred_2d = y_pred.reshape(-1, 3)\n",
    "\n",
    "y_test_inv = scaler.inverse_transform(y_test_2d)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred_2d)\n",
    "\n",
    "# you can now compute RMSE/MAE/MAPE per asset over all forecast steps\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "for i, asset in enumerate([\"Gold\", \"Silver\", \"CAD\"]):\n",
    "    r = rmse(y_test_inv[:, i], y_pred_inv[:, i])\n",
    "    a = mae(y_test_inv[:, i], y_pred_inv[:, i])\n",
    "    m = mape(y_test_inv[:, i], y_pred_inv[:, i])\n",
    "    print(f\"{asset} → RMSE: {r:.4f} | MAE: {a:.4f} | MAPE: {m:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d08a4a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (18672, 3), indices imply (437, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m test_start_idx = split  \u001b[38;5;66;03m# split from your earlier train/test split\u001b[39;00m\n\u001b[32m      3\u001b[39m test_timestamps = all_prices.index[test_start_idx + ENC_LEN : \n\u001b[32m      4\u001b[39m                                    test_start_idx + ENC_LEN + \u001b[38;5;28mlen\u001b[39m(y_test_inv)]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df_actual_test = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test_inv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGold\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSilver\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m df_pred_test = pd.DataFrame(\n\u001b[32m     13\u001b[39m     y_pred_inv,\n\u001b[32m     14\u001b[39m     index=test_timestamps,\n\u001b[32m     15\u001b[39m     columns=[\u001b[33m\"\u001b[39m\u001b[33mGold\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSilver\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCAD\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Python3.11/lib/python3.11/site-packages/pandas/core/frame.py:831\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    820\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    821\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    822\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    828\u001b[39m             copy=_copy,\n\u001b[32m    829\u001b[39m         )\n\u001b[32m    830\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Python3.11/lib/python3.11/site-packages/pandas/core/internals/construction.py:336\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[32m    332\u001b[39m index, columns = _get_axes(\n\u001b[32m    333\u001b[39m     values.shape[\u001b[32m0\u001b[39m], values.shape[\u001b[32m1\u001b[39m], index=index, columns=columns\n\u001b[32m    334\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Python3.11/lib/python3.11/site-packages/pandas/core/internals/construction.py:420\u001b[39m, in \u001b[36m_check_values_indices_shape_match\u001b[39m\u001b[34m(values, index, columns)\u001b[39m\n\u001b[32m    418\u001b[39m passed = values.shape\n\u001b[32m    419\u001b[39m implied = (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Shape of passed values is (18672, 3), indices imply (437, 3)"
     ]
    }
   ],
   "source": [
    "# The test target windows correspond to these rows:\n",
    "test_start_idx = split  # split from your earlier train/test split\n",
    "test_timestamps = all_prices.index[test_start_idx + ENC_LEN : \n",
    "                                   test_start_idx + ENC_LEN + len(y_test_inv)]\n",
    "\n",
    "df_actual_test = pd.DataFrame(\n",
    "    y_test_inv,\n",
    "    index=test_timestamps,\n",
    "    columns=[\"Gold\", \"Silver\", \"CAD\"]\n",
    ")\n",
    "\n",
    "df_pred_test = pd.DataFrame(\n",
    "    y_pred_inv,\n",
    "    index=test_timestamps,\n",
    "    columns=[\"Gold\", \"Silver\", \"CAD\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7310c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def forecast_interval(model, all_prices, scaler, ENC_LEN, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Autoregressive forecasting with a trained Transformer encoder–decoder.\n",
    "    \n",
    "    model      : trained Keras model\n",
    "    all_prices : DataFrame with flat columns ['Gold','Silver','CAD'] and DatetimeIndex\n",
    "    scaler     : fitted MinMaxScaler (for 3 price columns)\n",
    "    ENC_LEN    : encoder window length (same used in training)\n",
    "    start_date : str or pd.Timestamp\n",
    "    end_date   : str or pd.Timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date   = pd.to_datetime(end_date)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Compute prediction horizon (# of 30-min steps)\n",
    "    # ------------------------------------------------------------------\n",
    "    future_index = pd.date_range(start=start_date, end=end_date, freq=\"30min\")\n",
    "    DEC_LEN = len(future_index)\n",
    "\n",
    "    if DEC_LEN <= 0:\n",
    "        raise ValueError(\"Invalid date interval: DEC_LEN = 0\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. Get the encoder past window before start_date\n",
    "    # ------------------------------------------------------------------\n",
    "    if start_date not in all_prices.index:\n",
    "        raise ValueError(\"start_date is outside the data index\")\n",
    "\n",
    "    # The last ENC_LEN rows before start_date\n",
    "    enc_start = all_prices.index.get_loc(start_date) - ENC_LEN\n",
    "    if enc_start < 0:\n",
    "        raise ValueError(\"Not enough history before start_date\")\n",
    "\n",
    "    past_prices = all_prices.iloc[enc_start:enc_start+ENC_LEN]\n",
    "    \n",
    "    # Scale prices\n",
    "    past_scaled = scaler.transform(past_prices[['Gold','Silver','CAD']].values)\n",
    "\n",
    "    # Build time features\n",
    "    past_time_features = build_time_features(past_prices.index)\n",
    "\n",
    "    # Encoder input: concat prices + time features\n",
    "    enc_input = np.concatenate([past_scaled, past_time_features], axis=1)  # (ENC_LEN, 11)\n",
    "    enc_input = np.expand_dims(enc_input, axis=0)  # (1, ENC_LEN, 11)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. Prepare autoregressive decoder loop\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # Build the future time embeddings (we know timestamps)\n",
    "    future_time_features = build_time_features(future_index)  # (DEC_LEN, 8)\n",
    "\n",
    "    # Initialize decoder input\n",
    "    # At step 0: price = last encoder price\n",
    "    last_price = past_scaled[-1]  # shape (3,)\n",
    "    \n",
    "    dec_input_prices = []\n",
    "    dec_outputs = []\n",
    "\n",
    "    prev_price = last_price.copy()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. Autoregressive decoding\n",
    "    # ------------------------------------------------------------------\n",
    "    for step in range(DEC_LEN):\n",
    "\n",
    "        # Decoder input at current step:\n",
    "        #   prev_price + future_time_features[step]\n",
    "        dec_in_step = np.concatenate([prev_price, future_time_features[step]], axis=0)\n",
    "        dec_input_prices.append(dec_in_step)\n",
    "\n",
    "        # Convert accumulated decoder steps to array (1, step+1, 11)\n",
    "        dec_input_arr = np.array(dec_input_prices).reshape(1, -1, 11)\n",
    "\n",
    "        # FIX: pad decoder input to full DEC_LEN with zeros (model expects fixed length)\n",
    "        pad_len = DEC_LEN - dec_input_arr.shape[1]\n",
    "        if pad_len > 0:\n",
    "            dec_input_arr = np.concatenate(\n",
    "                [dec_input_arr, np.zeros((1, pad_len, 11))],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Predict full horizon, take only the first (step) prediction\n",
    "        pred = model.predict([enc_input, dec_input_arr], verbose=0)[0][step]\n",
    "\n",
    "        # Save prediction\n",
    "        dec_outputs.append(pred)\n",
    "\n",
    "        # Update prev_price for next step\n",
    "        prev_price = pred.copy()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. Inverse scale predictions\n",
    "    # ------------------------------------------------------------------\n",
    "    preds_scaled = np.array(dec_outputs)  # (DEC_LEN, 3)\n",
    "    preds = scaler.inverse_transform(preds_scaled)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6. Build output DataFrame\n",
    "    # ------------------------------------------------------------------\n",
    "    df_pred = pd.DataFrame(\n",
    "        preds,\n",
    "        index=future_index,\n",
    "        columns=['Gold', 'Silver', 'CAD']\n",
    "    )\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c926a97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "start_date is outside the data index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_pred = \u001b[43mforecast_interval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_prices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mENC_LEN\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2025-11-28 00:00:00\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2025-11-30 23:30:00\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_pred.head())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mforecast_interval\u001b[39m\u001b[34m(model, all_prices, scaler, ENC_LEN, start_date, end_date)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 2. Get the encoder past window before start_date\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_date \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_prices.index:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mstart_date is outside the data index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# The last ENC_LEN rows before start_date\u001b[39;00m\n\u001b[32m     35\u001b[39m enc_start = all_prices.index.get_loc(start_date) - ENC_LEN\n",
      "\u001b[31mValueError\u001b[39m: start_date is outside the data index"
     ]
    }
   ],
   "source": [
    "df_pred = forecast_interval(\n",
    "    model,\n",
    "    all_prices,\n",
    "    scaler,\n",
    "    ENC_LEN=60,\n",
    "    start_date=\"2025-11-28 00:00:00\",\n",
    "    end_date=\"2025-11-30 23:30:00\"\n",
    ")\n",
    "\n",
    "print(df_pred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c1af526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_forecast_with_baseline(\n",
    "    actual_df,     # true prices (DataFrame with DateTimeIndex)\n",
    "    predicted_df,  # predicted prices (same index)\n",
    "    asset=\"Gold\"   # column name\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots actual vs predicted vs naive baseline for a given asset.\n",
    "    actual_df    : DataFrame with actual prices, indexed by Datetime\n",
    "    predicted_df : DataFrame with predicted prices (same index)\n",
    "    asset        : One of 'Gold', 'Silver', 'CAD'\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1. Extract series\n",
    "    # -----------------------------------------------------------\n",
    "    actual = actual_df[asset]\n",
    "    predicted = predicted_df[asset]\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2. Baseline (persistence model)\n",
    "    # baseline[t] = actual[t-1]\n",
    "    # First baseline value = NaN, so shift actual by 1\n",
    "    # -----------------------------------------------------------\n",
    "    baseline = actual.shift(1)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3. Plot\n",
    "    # -----------------------------------------------------------\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.plot(actual.index, actual.values, \n",
    "             color=\"blue\", label=\"Actual Price\", linewidth=2)\n",
    "\n",
    "    plt.plot(predicted.index, predicted.values, \n",
    "             color=\"orange\", label=\"Predicted Price (Transformer)\", linewidth=2)\n",
    "\n",
    "    plt.plot(baseline.index, baseline.values, \n",
    "             color=\"magenta\", label=\"Baseline (Persistence)\", linewidth=2, linestyle=\"--\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4. Labels & formatting\n",
    "    # -----------------------------------------------------------\n",
    "    plt.title(f\"{asset} Price – Actual vs Predicted vs Baseline\", fontsize=16)\n",
    "    plt.xlabel(\"Time and Date\", fontsize=14)\n",
    "    plt.ylabel(\"Price in US Dollars\", fontsize=14)\n",
    "\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42cdbbcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Suppose you predicted Gold for a specific 30-min interval:\u001b[39;00m\n\u001b[32m      3\u001b[39m plot_forecast_with_baseline(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     actual_df = all_prices.loc[\u001b[43mdf_pred\u001b[49m.index],\n\u001b[32m      5\u001b[39m     predicted_df = df_pred,\n\u001b[32m      6\u001b[39m     asset=\u001b[33m\"\u001b[39m\u001b[33mGold\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'df_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Suppose you predicted Gold for a specific 30-min interval:\n",
    "\n",
    "plot_forecast_with_baseline(\n",
    "    actual_df = all_prices.loc[df_pred.index],\n",
    "    predicted_df = df_pred,\n",
    "    asset=\"Gold\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3a214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
